{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "import os\n",
    "from nltk import sent_tokenize\n",
    "nlp = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from spacy.symbols import nsubjpass\n",
    "# \n",
    "# # '''\n",
    "# #  ('((\\\\bclaim[^\\b]+\\\\b|\\\\baccept[^\\b]+\\\\b).*(\\\\bresponsibility\\\\b))', 'claim'), # perp claimed/accepted responsibility\n",
    "# #     ('((\\\\bha[^\\b]+\\\\b)+ *(\\\\bkilled\\\\b))', 'kill'), # perp has killed\n",
    "# #     ('(were +arrested)', 'arrest'), # perp were arrested\n",
    "# #     ('(blamed) +', 'blame'), # blamed _perp\n",
    "# #     (\"massacred\", 'massacre'),\n",
    "# #     ('kidnapped', 'kidnap'),\n",
    "# #     ('kidnapped by', 'kidnap'),\n",
    "# #     ('carried out by', 'carried'),\n",
    "# #     ('carried out', 'carried'),\n",
    "# #     ('(killed by)', 'kill'), # killed by. does not perform well\n",
    "# #     ('(^(we[^\\b]+) *destroyed)', 'destroy'),\n",
    "# #     ('(attacked)', 'attack'),\n",
    "# #     (\"guerrilla['s]? of\", '-'),\n",
    "# #     (\"member['s]? of\", '-'),\n",
    "# #     ('threatened', 'threaten'),\n",
    "# #     ('ambushed', 'ambush')\n",
    "# # '''\n",
    "# \n",
    "# matching_rules = [\n",
    "#     ('murder', [{'LEMMA': 'be'}, {'LEMMA': 'murder', 'OP': '*'}, {'POS': 'ADP', 'OP': '*'}]),\n",
    "#     # ('kidnap', [{'LEMMA': 'be'}, {'LEMMA': 'kidnap'}]),\n",
    "#     # ('killed', [{'LEMMA': 'be'}, {'LEMMA': 'killed'}]),\n",
    "#     # ('murdered', [{'ORTH': 'murdered'}]),\n",
    "#     # ('massacre', [{'ORTH': 'to', 'OP': '!'},{'ORTH': 'massacre'}])\n",
    "# ]\n",
    "# \n",
    "# output = []\n",
    "# \n",
    "# # Callback for matcher. gets the sentences and start-end position of matches.\n",
    "# def collect_sents(matcher, doc, i, matches):\n",
    "#     match_id, start, end = matches[i]\n",
    "#     span = doc[start : end]\n",
    "#     sent = span.sent\n",
    "#     match_ents = {'start': span.start-sent.start, 'end': span.end-sent.start,\n",
    "#                    'label': doc.vocab.strings[match_id]}\n",
    "#     output.append({'text': sent.text, 'ents': match_ents})\n",
    "#     return output\n",
    "#      \n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# \n",
    "# [matcher.add(rule[0], collect_sents, rule[1]) for rule in matching_rules] \n",
    "# \n",
    "# for i,line in enumerate(content[0:5000]):\n",
    "#     output = []\n",
    "#     doc = nlp(line)\n",
    "#     person_list = [ent for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "#     noun_chunk = []\n",
    "#     for noun in doc.noun_chunks:\n",
    "#         noun_chunk.append(noun)\n",
    "#     matches = matcher(doc)\n",
    "#     final_output = []\n",
    "#     if len(output) > 0:    \n",
    "#         # print(i)\n",
    "#         rule_label = output[0]['ents']['label']\n",
    "#         # print(line)\n",
    "#         # print(len(noun_chunk), noun_chunk)\n",
    "#         noun_clauses = []\n",
    "#         for token in doc:\n",
    "#             if rule_label in token.head.text and token.head.pos_ is not 'NOUN':\n",
    "#                 # print(token.text, token.pos_)\n",
    "#                 if 'dobj' in token.dep_:\n",
    "#                     a = prep_pobj_travel(token)\n",
    "#                     if a is not None:\n",
    "#                         # print(a)\n",
    "#                         # print([x for x in noun_chunk if a in x])\n",
    "#                         # print()\n",
    "#                         noun_clauses.extend([x.text for x in noun_chunk if a in x])\n",
    "#                     else:\n",
    "#                         noun_clauses.extend([x.text for x in noun_chunk if token in x])\n",
    "#                 # if 'PROPN' in token.pos_:\n",
    "#                 #     noun_clauses.extend([x for x in noun_chunk if token in x])\n",
    "#                 # if token.pos_ == 'NOUN' and 'prep' in [x.dep_ for x in token.children]:\n",
    "#                 #     for x in token.children:\n",
    "#                 #         if 'PREP' in token.dep_:\n",
    "#                 #             noun_clauses.append(corr_np_string(print_noun_recursive(token)))\n",
    "# \n",
    "#             \n",
    "#         if rule_label in line:\n",
    "#             for ent in doc.ents:\n",
    "#                 if ent.label_ == 'PERSON':\n",
    "#                     noun_clauses.append(ent.text)\n",
    "#                 # print(ent.text, ent.label_)\n",
    "#             \n",
    "#         # rule_pos = line.find(rule_label)\n",
    "#         # noun_clauses = [(abs(line.find(n) - rule_pos), n) for n in noun_clauses]\n",
    "#         # print(noun_clauses)\n",
    "#                     \n",
    "#         if len(noun_clauses) > 0:\n",
    "#             print('Matched Rule:', rule_label)\n",
    "#             print('Matched Line: ', line.upper())\n",
    "#             print(\"Victims: \", noun_clauses)\n",
    "#             print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = os.path.abspath('merged_project_ips.txt')\n",
    "with open(input_data, 'r') as f:\n",
    "    read_content = f.read()\n",
    "content = [c.replace('\\n', ' ').lower() for c in sent_tokenize(read_content)]\n",
    "read_content = read_content.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = []\n",
    "\n",
    "match_pattern_kidnap = [{'LEMMA': 'kidnap', 'POS':'NOUN'}] \n",
    "match_pattern_responsibility_claim = [{'LEMMA': 'claim', 'OP': '+'}, {'ORTH': 'responsibility'}]\n",
    "match_pattern_responsibility_accept = [{'LEMMA': 'accept', 'OP': '+'}, {'ORTH': 'responsibility'}]\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start : end] # matched span\n",
    "    sent = span.sent # sentence containing matched span\n",
    "    # append mock entity for match in displaCy style to matched_sents\n",
    "    # get the match span by ofsetting the start and end of the span with the\n",
    "    # start and end of the sentence in the doc\n",
    "    match_ents = [{'start': span.start-sent.start, 'end': span.end-sent.start,\n",
    "                   'label': 'MATCH'}]\n",
    "    output.append({'text': sent.text, 'ents': match_ents })    \n",
    "\n",
    "from spacy.symbols import nsubjpass\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add('murder_event', collect_sents, match_pattern_responsibility_claim)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for o in output:\n",
    "    # print(o)\n",
    "    print()\n",
    "if len(output) > 0:    \n",
    "    op = output[0]['text']\n",
    "    \n",
    "    sent_parse = nlp(op)\n",
    "    \n",
    "    for token in sent_parse:\n",
    "        if token.dep == nsubjpass:\n",
    "            print(corr_np_string(print_noun_recursive(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex_rules = [\n",
    "    ('((\\\\bclaim[^\\b]+\\\\b|\\\\baccept[^\\b]+\\\\b).*(\\\\bresponsibility\\\\b))', 'claim'), # perp claimed/accepted responsibility\n",
    "    ('((\\\\bha[^\\b]+\\\\b)+ *(\\\\bkilled\\\\b))', 'kill'), # perp has killed\n",
    "    ('(were +arrested)', 'arrest'), # perp were arrested\n",
    "    ('(blamed) +', 'blame'), # blamed _perp\n",
    "    (\"massacred\", 'massacre'),\n",
    "    ('kidnapped', 'kidnap'),\n",
    "    ('kidnapped by', 'kidnap'),\n",
    "    ('carried out by', 'carried'),\n",
    "    ('carried out', 'carried'),\n",
    "    ('(killed by)', 'kill'), # killed by. does not perform well\n",
    "    ('(^(we[^\\b]+) *destroyed)', 'destroy'),\n",
    "    ('(attacked)', 'attack'),\n",
    "    (\"guerrilla['s]? of\", '-'),\n",
    "    (\"member['s]? of\", '-'),\n",
    "    ('threatened', 'threaten'),\n",
    "    ('ambushed', 'ambush')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perp_org(text):\n",
    "    for line in text:\n",
    "        b = line.lower()\n",
    "        for i,r in enumerate([x[0] for x in regex_rules]):\n",
    "            a = re.findall(r, b)\n",
    "            if len(a) > 0:\n",
    "                sent = line\n",
    "                line_parse = nlp(sent)\n",
    "                orgs = [x for x in line_parse.ents if x.label_ == \"ORG\"]\n",
    "                if len(orgs) > 0:\n",
    "                    orgs = [(x.text, x.start) for x in orgs]\n",
    "                    reg_pos = line.lower().find(a[0][0])\n",
    "                    orgs = [(x[0], reg_pos - x[1]) for x in orgs]\n",
    "                    print(orgs)\n",
    "                    return min(orgs, key=lambda x: x[1])\n",
    "               \n",
    "               \n",
    "def perp_ind(line):\n",
    "    b = line.lower()\n",
    "    for i, r in enumerate([x[0] for x in regex_rules]):\n",
    "        a = re.findall(r, b)\n",
    "        if len(a) > 0:\n",
    "            sent = line\n",
    "            line_parse = nlp(sent)\n",
    "            # inds = [x for x in line_parse if 'nsubj' in x.dep_]\n",
    "            inds = []\n",
    "            for x in line_parse:\n",
    "                if x.pos_ == 'VERB' and regex_rules[i][1] in x.text:\n",
    "                    for c in x.children:\n",
    "                        if 'obj' in c.dep_:\n",
    "                            inds.append(corr_np_string(print_noun_recursive(c)))\n",
    "                    \n",
    "            if len(inds) > 0:\n",
    "                print(inds)\n",
    "                print(r)\n",
    "                print(line)\n",
    "                print()\n",
    "                return inds\n",
    "            # if len(inds) > 0:\n",
    "            #     inds = [(x.text, x.start) for x in inds]\n",
    "            #     reg_pos = line.lower().find(a[0][0])\n",
    "            #     inds = [(x[0], reg_pos - x[1]) for x in inds]\n",
    "            #     return min(inds, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_noun_recursive(token):\n",
    "    output = [token.text]\n",
    "    for c in token.children:\n",
    "        output.extend(print_noun_recursive(c))\n",
    "    return list(reversed(output))\n",
    "\n",
    "def corr_np_string(np_chunk):\n",
    "    return \" \".join(np_chunk)\n",
    "\n",
    "def prep_pobj_travel(token):\n",
    "    for c in token.children:\n",
    "        if c.dep_ is 'prep':\n",
    "            for c_ in c.children:\n",
    "                if c_.dep_ is 'pobj':\n",
    "                    return c_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_answers():\n",
    "    answers = 'test_merged_ans.txt'\n",
    "    with open(answers, 'r') as f:\n",
    "        answers = f.readlines()\n",
    "    victim_dict = {}\n",
    "    curr_id = \"\"\n",
    "    is_vic = False\n",
    "    for ans in answers:\n",
    "        if 'ID:' in ans:\n",
    "            is_vic = False\n",
    "            curr_id = ans.split(':')[1].strip()\n",
    "        elif 'VICTIM:' in ans or is_vic:\n",
    "            is_vic = True\n",
    "            a = \"\"\n",
    "            if ':' in ans:\n",
    "                a = ans.split(':')[1].strip()\n",
    "            else:\n",
    "                a = ans.strip()\n",
    "            \n",
    "            if curr_id in victim_dict.keys():\n",
    "                victim_dict[curr_id].append(a)\n",
    "            else:\n",
    "                victim_dict[curr_id] = [a]\n",
    "        elif ':' in ans:\n",
    "            is_vic = False\n",
    "    victim_dict = {key: [x for x in value if len(x.strip()) > 1 or x == '-'] for (key, value) in victim_dict.items()}\n",
    "    return victim_dict\n",
    "\n",
    "victim_dictionary = read_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def read_files():\n",
    "    text_files = os.path.abspath('test_merged_art.txt')\n",
    "    id_match = re.compile('[A-Z]{3}[1-9]*-[A-Z]{3}[1-9]*-[0-9]{4}')\n",
    "    id_stuff = re.compile('\\(.*?\\)')\n",
    "    start_regex = re.compile('.*\\[.*?\\]')\n",
    "    with open(text_files, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "    content = [c.replace('\\n', ' ').strip() for c in content]\n",
    "    \n",
    "    file_dict = {}    \n",
    "    \n",
    "    curr_file_id = \"\"\n",
    "    file = []\n",
    "    for line in content:\n",
    "        if id_match.match(line):\n",
    "            if len(file) > 0:\n",
    "                file = \" \".join(file)\n",
    "                file_dict[curr_file_id] = file.strip()\n",
    "                file = []\n",
    "            stuff = id_stuff.findall(line)\n",
    "            for s in stuff:\n",
    "                line = line.replace(s, '')\n",
    "            curr_file_id = line.strip()\n",
    "        else:\n",
    "            if start_regex.findall(line):\n",
    "                line = line.replace(start_regex.findall(line)[0], '')\n",
    "            file.append(line.strip())\n",
    "    return file_dict\n",
    "\n",
    "file_dict = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "        match_id, start, end = matches[i]\n",
    "        span = doc[start : end]\n",
    "        sent = span.sent\n",
    "        match_ents = {'start': span.start-sent.start, 'end': span.end-sent.start,\n",
    "                       'label': doc.vocab.strings[match_id]}\n",
    "        output.append({'text': sent.text, 'ents': match_ents})\n",
    "        return output\n",
    "\n",
    "\n",
    "def get_victim(line_l, nlp, Matcher):\n",
    "    global output\n",
    "    output = []\n",
    "    line = line_l.lower().replace('\\n', ' ')\n",
    "    \n",
    "    keep_char_regex = re.compile('^[\\W\\s]')\n",
    "    line = keep_char_regex.sub('', line)\n",
    "    matching_rules = [\n",
    "        ('murder_passive', [{'LEMMA': 'be'}, {'LEMMA': 'murder', 'OP': '*'}, {'POS': 'ADP', 'OP': '*'}]),\n",
    "        ('kidnap', [{'LEMMA': 'be'}, {'LEMMA': 'kidnap'}]),\n",
    "        ('killed', [{'LEMMA': 'be'}, {'LEMMA': 'killed'}]),\n",
    "        ('murder_active', [{'LEMMA': 'be', 'OP': '!'}, {'ORTH': 'murdered'}]),\n",
    "        ('massacre', [{'ORTH': 'to', 'OP': '!'}, {'ORTH': 'massacre'}]),\n",
    "        ('wounded_passive', [{'LEMMA': 'be'}, {'LEMMA': 'wound'}])\n",
    "    ]\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    [matcher.add(rule[0], collect_sents, rule[1]) for rule in matching_rules]\n",
    "\n",
    "    doc = nlp(line)\n",
    "    noun_chunk = []\n",
    "    for noun in doc.noun_chunks:\n",
    "        noun_chunk.append(noun.text)\n",
    "    matcher(doc)\n",
    "    final_nouns = []\n",
    "    if len(output) > 0:\n",
    "        rule_label = output[0]['ents']['label']\n",
    "        noun_clauses = []\n",
    "\n",
    "        for i,token in enumerate(doc):\n",
    "            if 'jesuit' in token.text and 'priests' in doc[i+1].text:\n",
    "                final_nouns.append('JESUIT PRIESTS')\n",
    "            elif 'priest' in token.text:\n",
    "                final_nouns.append('PRIESTS')\n",
    "            elif 'jesuit' in token.text:\n",
    "                final_nouns.append('JESUIT')\n",
    "            else:\n",
    "                if 'murder' in token.text:\n",
    "                    if rule_label == 'murder_active':\n",
    "                        noun_clauses.extend([x.text for x in token.rights])\n",
    "                if 'wounded' in token.text:\n",
    "                    noun_clauses.extend([x.text for x in token.rights])\n",
    "                \n",
    "        if 'murder' in line:\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'PERSON':\n",
    "                    for x in noun_chunk:\n",
    "                        if ent.text in x or x in ent.text:\n",
    "                            if len(x) > len(ent.text):\n",
    "                                final_nouns.append(x)\n",
    "                            else:\n",
    "                                final_nouns.append(ent.text)\n",
    "\n",
    "        if 'kidnap' in line:\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'PERSON':\n",
    "                    for x in noun_chunk:\n",
    "                        if ent.text in x or x in ent.text:\n",
    "                            if len(x) > len(ent.text):\n",
    "                                final_nouns.append(x)\n",
    "                            else:\n",
    "                                final_nouns.append(ent.text)\n",
    "\n",
    "        if 'kill' in line:\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'PERSON':\n",
    "                    for x in noun_chunk:\n",
    "                        if ent.text in x or x in ent.text:\n",
    "                            if len(x) > len(ent.text):\n",
    "                                final_nouns.append(x)\n",
    "                            else:\n",
    "                                final_nouns.append(ent.text)\n",
    "                                \n",
    "        if 'massacre' in line:\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'PERSON':\n",
    "                    for x in noun_chunk:\n",
    "                        if ent.text in x or x in ent.text:\n",
    "                            if len(x) > len(ent.text):\n",
    "                                final_nouns.append(x)\n",
    "                            else:\n",
    "                                final_nouns.append(ent.text)\n",
    "        \n",
    "        for n in noun_clauses:\n",
    "            for x in noun_chunk:\n",
    "                if n in x:\n",
    "                    final_nouns.append(x)\n",
    "\n",
    "        final_nouns = [x.upper().strip() for x in final_nouns]\n",
    "\n",
    "        if len(final_nouns) > 0:\n",
    "            return final_nouns\n",
    "    return ['-']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "victim_ouputs = {}\n",
    "for file in file_dict:\n",
    "    vics = []\n",
    "    for line in sent_tokenize(file_dict[file]):\n",
    "        v = get_victim(line, nlp, Matcher)\n",
    "        vics.extend(v)\n",
    "    victim_ouputs[file] = list(set(vics))\n",
    "\n",
    "for key, value in victim_dictionary.items():\n",
    "    if key in victim_ouputs.keys():\n",
    "        print(value)\n",
    "        print(victim_ouputs[key])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-105-0911a33560f1>, line 4)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-105-0911a33560f1>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    annotations = annotator.getAnnotations(line)\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for file in file_dict:\n",
    "    vics = []\n",
    "    for line in sent_tokenize(file_dict[file]):\n",
    "\t\tannotations = annotator.getAnnotations(line)\n",
    "\t\tprint(annotations)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
